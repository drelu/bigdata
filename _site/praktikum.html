
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Praktikum</title>
    <meta name="description" content="">
    <meta name="author" content="Andre Luckow">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="/assets/themes/twitter/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="/assets/themes/twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->
  <!-- Update these with your own images
    <link rel="shortcut icon" href="images/favicon.ico">
    <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
  -->
  </head>

  <body>

    <div class="navbar">
      <div class="navbar-inner">
        <div class="container">
          <a class="brand" href="/">Big Data und Large Scale Systems</a>
          <ul class="nav">
            
            
            


  
    
      
    
  
    
      
      	
      	<li><a href="/contact.html">Kontakt</a></li>
      	
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/pages.sv">Pages</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  



          </ul>
        </div>
      </div>
    </div>

    <div class="container">

      <div class="content">
        
<div class="page-header">
  <!--h1>Praktikum </h1-->
  <h1>Praktikum </h1>
</div>

<div class="row">
  <div class="span12">
    <h1 id='infrastruktur'>Infrastruktur</h1>

<ul>
<li>
<p>Linux Cluster hosted at Amazon EC2</p>
</li>

<li>
<p>Ubuntu 12.04</p>
</li>

<li>
<p>Python 2.7.3, Python Dokumentation: <a href='http://docs.python.org/'>http://docs.python.org/</a></p>
</li>

<li>
<p>Hostname: <code>cloud.luckow-hm.de</code></p>
</li>

<li>
<p>Amazon Console: <a href='https://bigdata.signin.aws.amazon.com/console'>https://bigdata.signin.aws.amazon.com/console</a></p>
</li>

<li>
<p><a href='http://netzmafia.de/skripten/unix/index.html'>Netzmafia Einführung Linux</a></p>
</li>

<li>
<p>User werden beim ersten Praktikum am 12.10.2012 vergeben!</p>
</li>
</ul>
<br />
<h1 id='1_umgang_mit_ssh_und_linux'>1. Umgang mit SSH und Linux</h1>

<ol>
<li>
<p>Loggen Sie sich mit SSH auf der Linux VM ein! Ändern Sie ihr Passwort!</p>
</li>

<li>
<p>Beantworten Sie folgende Fragen:</p>

<ul>
<li>Wie viele User sind aktuell auf dem Server angelegt? Wie viele sind derzeit angemeldet?</li>

<li>Wie viel Plattenspeicher ist verfügbar?</li>

<li>Wann wurde der Rechner das letzte Mal gestartet?</li>

<li>Welcher Prozess hat aktuell den höchsten CPU/Speicher Verbrauch?</li>
</ul>
</li>

<li>
<p>Gehen Sie durch das Python Tutorial: * http://docs.python.org/tutorial/introduction.html</p>
</li>
</ol>
<br />
<h1 id='2_restful_cloud_apis_am_beispiel_von_twitter'>2. RESTful Cloud APIs am Beispiel von Twitter</h1>

<ol>
<li>
<p>Erstellen Sie in einer Programmiersprache ihre Wahl einen Twitter Client. Der Client soll:</p>

<ul>
<li>die aktuelle Public Timeline und</li>

<li>in der Nähe gepostet Tweets</li>
</ul>

<p>anzeigen können.</p>
</li>

<li>
<p>Zusatzaufgabe: Nutzen Sie eine OAuth Bibliothek, um User gegenüber Twitter authentisieren und autorisieren.</p>

<p>Material:</p>

<ul>
<li>Twitter Developer Site, <a href='http://dev.twitter.com/'>http://dev.twitter.com/</a></li>

<li>Twitter Streaming API Präsentation, <a href='http://www.slideshare.net/jkalucki/thinking-in-streaming-twitter-streaming-api'>http://www.slideshare.net/jkalucki/thinking-in-streaming-twitter-streaming-api</a></li>

<li>Tweetstream Package: <a href='http://pypi.python.org/pypi/tweetstream'>http://pypi.python.org/pypi/tweetstream</a></li>
</ul>
</li>
</ol>
<br />
<h1 id='3_datenanalyse_mit_kommandozeile_und_python'>3. Datenanalyse mit Kommando-Zeile und Python</h1>
<br />
<p>Notwendige Daten/Tools</p>

<ul>
<li><a href='http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html'>http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html</a></li>

<li>Commandline data tools <a href='https://github.com/bitly/data_hacks'>https://github.com/bitly/data_hacks</a></li>

<li>Daten: <code>cloud.luckow-hm.de:/data/NASA_access_log_Jul95</code> <br /></li>
</ul>

<ol>
<li>
<p>Nutzen die Kommandos <code>head</code>, <code>cat</code>, <code>uniq</code>, <code>wc</code>, <code>sort</code>, <code>find</code>, <code>xargs</code>, <code>awk</code> um die NASA Access Logs auszuwerten:</p>

<ol>
<li>Welche Seiten wurden am meisten angesurft?</li>

<li>Welcher Fehler trat am Meisten auf?</li>

<li>Wie viele HTTP Fehler gab es insgesamt? Wieviel Prozent der Requests wurden mit einem Fehler beendet.</li>
</ol>
</li>
</ol>

<p><em>Lösung</em></p>

<pre><code>cat /data/NASA_access_log_Jul95 | awk  &#39;{print $(NF-1)}&#39;| sort | uniq -c</code></pre>
<br />
<h1 id='4_mapreducebasierte_logdateien_auswertung'>4. MapReduce-basierte Log-Dateien Auswertung</h1>
<br />
<p><em>Informationen:</em></p>

<ul>
<li>
<p>Hadoop Dokumentation: <a href='http://hadoop.apache.org/docs/r1.0.3/'>http://hadoop.apache.org/docs/r1.0.3/</a></p>
</li>

<li>
<p>Hadoop Help Pages:</p>

<pre><code>  hadoop fs -help 
  hadoop jar $HADOOP_HOME/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.1.2.jar -info</code></pre>
</li>

<li>
<p>Hadoop Web Schnittstellen:</p>

<p>* Namenode: <code>lynx http://localhost:50070</code></p>

<p>* Jobtracker: <code>lynx http://localhost:50030</code></p>
</li>

<li>
<p>Hadoop Home: <code>/usr/lib/hadoop-0.20-mapreduce/</code></p>
</li>

<li>
<p>Hadoop Streaming Bibliothek: <code>/usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.1.1.jar</code></p>
</li>

<li>
<p>Input Daten: <code>cloud.luckow-hm.de:/data/NASA_access_log_Jul95</code></p>
</li>

<li>
<p><a href='http://cdn.oreillystatic.com/en/assets/1/event/85/An%20Introduction%20to%20Hadoop%20Presentation.pdf'>Introduction to Hadoop</a></p>
</li>
</ul>
<br /><br />
<ol>
<li>
<p>Nutzen Sie das MapReduce Programmiermodell, um die Statistiken auf Aufgabe 3 zu erfolgen. Nutzen Sie das folgende <a href='src/map_reduce.py'>Python-Skript</a> als Template! Testen Sie das Skript:</p>

<pre><code> cat &lt;input file&gt; | python map_reduce.py map | sort | python map_reduce.py reduce</code></pre>
</li>

<li>
<p>Machen Sie sich mit dem Hadoop Dateisystem vertraut! Laden Sie die Eingabedateien für den MapReduce Job in das Hadoop Filesystem (auf <code>cloud.luckow-hm.de</code>). Legen Sie dazu ein neues Verzeichnis <code>input</code> in ihrem HDFS Home Verzeichnis an!</p>
</li>

<li>
<p>Lassen Sie ihr erstelltes Auswertungsskript mit Hadoop laufen. Beobachten Sie die Ausführungen: Wie viele Map Tasks werden generiert? Wie viel Map Slots belegt die Applikation?</p>
</li>

<li>
<p>Vergleichen Sie die Laufzeiten der lokalen Ausführung mit der Hadoop Variante. Erlären Sie den Unterschied!</p>
</li>
</ol>
<br />
<p><em>Lösung</em><br /></p>

<p><a href='src/nasa.py'>Python-Skript</a></p>
<br /><br />
<h1 id='5_getting_started_mit_amazon_ec2_und_s3'>5. Getting Started mit Amazon EC2 und S3</h1>
<br />
<p>S3 Dokumentation: <a href='http://aws.amazon.com/documentation/s3/'>http://aws.amazon.com/documentation/s3/</a> EC2 Dokumentation: <a href='http://aws.amazon.com/documentation/ec2/'>http://aws.amazon.com/documentation/ec2/</a></p>

<p><em>Bitte stellen Sie sicher, dass Sie alle Instanzen am Ende der Vorlesung beenden!</em></p>

<ol>
<li>
<p>Konfigurieren Sie die EC2 und S3 Tools mit ihrem Amazon Access und Secret Access Schlüssel!</p>
</li>

<li>
<p>Nutzen Sie <code>s3cmd</code>, um eine Datei ihrer Wahl auf S3 hochzuladen! Machen Sie die Datei Public! Prüfen Sie ob Sie die Datei über den Web-Browser erreichen können!</p>
</li>

<li>
<p>Erstellen Sie ein Python Programm unter Nutzung von <a href='http://docs.pythonboto.org/en/latest/index.html'>Boto</a>, das <code>/data</code> Verzeichnis auf Amazon S3 zu kopieren. Messen Sie die notwendige Zeit und berechnen Sie die Bandbreite!</p>
</li>

<li>
<p>Erstellen Sie EC2 Instanz (Typ: <code>t1.micro</code>). Folgen Sie dazu allen notwendigen Schritten (z.B. Erstellung eines Keys). Loggen Sie sich auf dieser mit SSH ein!</p>
</li>
</ol>
<br /><br />
<h1 id='6_amazons_elastic_mapreduce'>6. Amazon&#8217;s Elastic MapReduce</h1>
<br />
<p>Elastic MapReduce Dokumentation: <a href='http://docs.amazonwebservices.com/ElasticMapReduce/latest/GettingStartedGuide/Welcome.html'>http://docs.amazonwebservices.com/ElasticMapReduce/latest/GettingStartedGuide/Welcome.html</a></p>

<pre><code>Bitte legen Sie eine Datei `credentials.json`
{
    &quot;access_id&quot;: &quot;&quot;,
    &quot;private_key&quot;: &quot;&quot;,
    &quot;keypair&quot;: &quot;MyKey&quot;,
    &quot;key-pair-file&quot;: &quot;&lt;HOME&gt;/.ssh/id_rsa&quot;,
    &quot;log_uri&quot;: &quot;s3://emr-drelu-log&quot;,
    &quot;region&quot;: &quot;us-east-1&quot;
}</code></pre>
<!--TestDFSIO: <http://answers.oreilly.com/topic/460-how-to-benchmark-a-hadoop-cluster/>-->
<ol>
<li>
<p>Starten Sie einen 1 Knoten Hadoop Cluster mit Elastic MapReduce! Nutzen Sie dabei das Kommandozeilentool <code>elastic-mapreduce</code> und die Optionen <code>--hive-interactive</code> und <code>--alive</code>!</p>
</li>

<li>
<p>Loggen Sie sich auf den Master-Knoten ein! Machen Sie sich mit Hadoop vertraut (<code>hadoop help</code>)!</p>
</li>

<li>
<p>Testen Sie das HDFS! Wie viel Speicher ist im HDFS verfügbar?</p>
</li>

<li>
<p>Lassen Sie das Log-File Beispiel aus der vorherigen Übung auf dem Cluster laufen!</p>
</li>

<li>
<p>Generieren Sie durch kopieren der Daten im HDFS ein Input Datensatz von 10 GB! Messen Sie die Laufzeit auf 1 Knoten!</p>
</li>

<li>
<p>Stoppen Sie den JobFlow und starten Sie einen weiteren Jobflow mit 3 Knoten (d.h. 2 Data Nodes)! Wiederholen Sie das Experiment und messen Sie die Laufzeit!</p>
</li>

<li>
<p>Legen Sie die gleichen Input Daten auf Amazon S3 ab und wiederholen Sie das Experiment. Erklären Sie das Ergebnis!</p>
</li>
</ol>

<h1 id='7_hive'>7. Hive</h1>
<br />
<p>Hive Dokumentation: <a href='http://hive.apache.org/'>http://hive.apache.org/</a> <br /></p>

<ol>
<li>
<p>Starten Sie 1 EMR Jobflow mit 1 Knoten unter Nutzung des Kommandozeilentools <code>elastic-mapreduce</code> (Optionen: <code>--hive-interactive</code> und <code>--alive</code>)!</p>
</li>

<li>
<p>Machen Sie sich mit Hive vertraut in dem Sie sich auf der Kommandozeile auf den Master-Knoten des EMR-Clusters einloggen.!</p>
</li>

<li>
<p>Erstellen Sie für das National Climate Data Center Datensample eine Hive Tabelle. Laden Sie die Sample-Daten <code>cloud.luckow-hm.de:/data/ncdc/sample.txt</code> in diese Tabelle!</p>
</li>

<li>
<p>Erstellen Sie eine SQL Query die die Maximaltemperatur pro Jahr ausgibt! Lassen Sie diese laufen!</p>
</li>
</ol>
  </div>
</div>


      </div>

      <footer>
        <p>&copy; Andre Luckow 2012</p>
      </footer>

    </div> <!-- /container -->

    
  </body>
</html>

